{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet\n",
    "- [Turbo Fan Engine Degration Simulation Data Set (2008): NASA](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan)\n",
    "    - Recorded as Multiple MultiVariate Time Series\n",
    "    - 3  Settings: (setting{x}): operational environment condition values that change over time \n",
    "    - 21 Sensors   (s1-s21): Each Sensor measuring info about the physical state of a particular engine\n",
    "    - Each Engine  (id): Stars with different unknown level of wear and allowed to run until failure\n",
    "- Terms\n",
    "    - RUL = Remaining Useful Life (in units of hours or cycles)\n",
    "    - TUL = Time Until Failure\n",
    "- Background\n",
    "    - Sensor Readings from a Fleet of Simulated Gas Turbine Engines\n",
    "- Goal:\n",
    "    - Predict when an in-service engine will fail based on target (RUL, Label1, Label2)\n",
    "    - Determine period ahead to alert before failure is oncoming\n",
    "- Training Data\n",
    "    - Sensor measuerments recorded time steps up until failure occurs\n",
    "    - Every sample in the Training Set will fail => so able to predict TUL value at each time step\n",
    "    - Number of Sequences per Engine = Number of Cycles for each Engine until Failure\n",
    "    - Due to their different initial conditions, each engine has a slightly different lifetime and failure pattern\n",
    "    - Therefore each engine's progress in time is not quite aligned with any other (therefore cannot compare cycles)\n",
    "    - TUL allows to align different engines data to common endpoint (t=0)\n",
    "- Testing Data\n",
    "    - Does not indicate when failure occurs (last time period does not represent failure point)\n",
    "    - Ground Truth (RUL_FD00x.txt): provides # of remaining work cycles for engines in 'Testing Data' before failure\n",
    "    - Ground Truth used to generated labels\n",
    "- Labels:\n",
    "    - Let w0=15 (last 15 Cycles), w1=30 (last 30 cycles)\n",
    "    - Regression => per RUL\n",
    "    - Label1: Binary Classification => fail within (*w1* cycles) certain time frame\n",
    "    - Label2: Multi-Class Classification => fail within cycles: !within w1 days | *[1,w0]* | *[w0+1, w1]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Describe DataSet\n",
    "- Download [NASA TurboFan Dataset](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan)\n",
    "- 4 Different Experiments: Load a Particular Single Experiment\n",
    "- Describe the DataSet: How many unique engines, Are Input Records of Variable Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "root_dir  = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_root = os.path.join(root_dir, \"data\")\n",
    "data_dir  = os.path.join(data_root, \"nasa-turbofan\")\n",
    "data_file = os.path.join(data_dir,  \"turbofan.zip\")\n",
    "if not os.path.exists(data_dir): \n",
    "    os.makedirs(data_dir) \n",
    "    !wget  \"https://ti.arc.nasa.gov/c/6/\" -O \"../data/turbofan.zip\"\n",
    "    !unzip \"../data/turbofan.zip\" -d \"../data/nasa-turbofan\"\n",
    "    os.symlink(data_dir, (os.path.join(os.getenv('HOME'), 'Data/nasa-turbofan')), target_is_directory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each row is snapshot of data take during a single operation cycle (1 Cycle=21 Sensor Readings)\n",
    "def read_data(data_type, data_unit, num_readings=21, drop_cols=None):\n",
    "    \"\"\"\n",
    "    read in the appropriate dataset \n",
    "    data_type: {train, test, RUL}\n",
    "    data_unit: {001-004}\n",
    "    \"\"\"\n",
    "    #format:   one time series per file, separate files for labels \n",
    "    #record:   each record is a time step from readings of the sensors for a particular engine id\n",
    "    #id field: engine id (trajectories)\n",
    "    #cycle:    as time unit => number of operational cycles since start of engine operation \n",
    "    #s1-s21:   each of different sensor units providing readings \n",
    "    cols = ['id', 'cycle', 'setting1', 'setting2', 'setting3']\n",
    "    cols = cols + [\"\".join(['s',str(el)]) for el in range(1,num_readings+1)]\n",
    "\n",
    "    df = pd.read_csv(os.path.join(data_dir,\"{}_FD{}.txt\".format(data_type, data_unit)), header=None, sep=\" \")\n",
    "    df = df.drop(df.columns[drop_cols], axis=1)\n",
    "    df.columns = cols\n",
    "    df = df.sort_values(['id','cycle'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_data('train', '001', num_readings=21, drop_cols=[26, 27])\n",
    "df_test  = read_data('test',  '001', num_readings=21, drop_cols=[26, 27])\n",
    "df_rul   = pd.read_csv(os.path.join(data_dir,\"RUL_FD001.txt\"), header=None, sep =\" \").drop([1], axis=1)\n",
    "# TBD: Look at the different distributions of each dataset partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Shapes: Train: (20631, 26), Test: (13096, 26), RUL: (100, 1)\n",
      "Number of Engines: 101\n",
      "Variable Engine Time Series => \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cycle</th>\n",
       "      <td>192</td>\n",
       "      <td>287</td>\n",
       "      <td>179</td>\n",
       "      <td>189</td>\n",
       "      <td>269</td>\n",
       "      <td>188</td>\n",
       "      <td>259</td>\n",
       "      <td>150</td>\n",
       "      <td>201</td>\n",
       "      <td>222</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>341</td>\n",
       "      <td>155</td>\n",
       "      <td>258</td>\n",
       "      <td>283</td>\n",
       "      <td>336</td>\n",
       "      <td>202</td>\n",
       "      <td>156</td>\n",
       "      <td>185</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id     1    2    3    4    5    6    7    8    9    10  ...   91   92   93   \\\n",
       "cycle  192  287  179  189  269  188  259  150  201  222 ...   135  341  155   \n",
       "\n",
       "id     94   95   96   97   98   99   100  \n",
       "cycle  258  283  336  202  156  185  200  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5   ...        s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
       "0  14.62   ...     521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n",
       "1  14.62   ...     522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n",
       "2  14.62   ...     522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n",
       "3  14.62   ...     522.86  2388.08  8133.83  8.3682  0.03  392  2388  100.0   \n",
       "4  14.62   ...     522.19  2388.04  8133.80  8.4294  0.03  393  2388  100.0   \n",
       "\n",
       "     s20      s21  \n",
       "0  39.06  23.4190  \n",
       "1  39.00  23.4236  \n",
       "2  38.95  23.3442  \n",
       "3  38.88  23.3739  \n",
       "4  38.90  23.4044  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Partition Shapes: Train: {}, Test: {}, RUL: {}\".format(df_train.shape, df_test.shape, df_rul.shape))\n",
    "print(\"Number of Engines: {}\".format(df_rul.shape[0] + 1))\n",
    "print(\"Variable Engine Time Series => \")\n",
    "display(pd.DataFrame(df_train.groupby('id')['cycle'].count()).T)\n",
    "df_train[df_train.id == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Labels\n",
    "- Need to indicate target for Binary Classification Problem to Solve on Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_maxcycles(df):\n",
    "    \"\"\"\n",
    "    calculates an engines max cycles RUL \n",
    "    \"\"\"\n",
    "    # acquire maximum cycle until failure of an engine \n",
    "    # for each cycle determine delta (RUL) to remaining cycles until failure\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    return rul\n",
    "\n",
    "def update_maxcycles(df_cycles, df_gt):\n",
    "    \"\"\"\n",
    "    used during test dataset to update the RUL value using the GT (df_rul)\n",
    "    only used for test set, as the number of cycles is not indicative till failure\n",
    "    \"\"\"\n",
    "    df_gt_link = df_gt.copy()\n",
    "    df_gt_link.columns = ['more']\n",
    "    df_gt_link['id']   = df_gt_link.index + 1\n",
    "    df_gt_link['max']  = df_cycles['max'] + df_gt_link['more']\n",
    "    df_gt_link = df_gt_link.drop('more', axis=1)\n",
    "    return df_gt_link\n",
    "\n",
    "\n",
    "def calc_rul(df, df_cycles):\n",
    "    \"\"\"\n",
    "    calculates RUL based on maximum yccles of an engine\n",
    "    used for regression prediction\n",
    "    \"\"\"\n",
    "    df = df.merge(df_cycles, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] - df['cycle']\n",
    "    df = df.drop('max', axis=1)\n",
    "    return df\n",
    "    \n",
    "def calc_labels(df, w0=15, w1=30):\n",
    "    \"\"\"\n",
    "    calculate labels (binary, multiclass) based on RUL falling within the tail of a cycle window\n",
    "    \"\"\"\n",
    "    df['label1'] = np.where(df['RUL'] <= w1, 1, 0 )\n",
    "    df['label2'] = df['label1']\n",
    "    df.loc[df['RUL'] <= w0, 'label2'] = 2\n",
    "    return df\n",
    "\n",
    "def scale_features(scaler, df, cols, phase):\n",
    "    \"\"\"\n",
    "    scale certain parameters: cycle_norm\n",
    "    \"\"\"\n",
    "    df['cycle_norm'] = df['cycle']\n",
    "    cols_normalize = df.columns.difference(cols)\n",
    "    fn = scaler.fit_transform if phase == 'train' else scaler.transform\n",
    "    df_norm = pd.DataFrame(fn(df[cols_normalize]), \n",
    "                           columns=cols_normalize, \n",
    "                           index=df.index)\n",
    "    df_join = df[df.columns.difference(cols_normalize)].join(df_norm)\n",
    "    df = df_join.reindex(columns = df.columns).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "id_cols     = ['id', 'cycle']\n",
    "target_cols = ['RUL','label1','label2']\n",
    "scaler_norm = preprocessing.StandardScaler()\n",
    "\n",
    "df_train_cycles  = calc_maxcycles(df_train)\n",
    "df_train_rul     = calc_rul(df_train, df_train_cycles)\n",
    "df_train_labeled = calc_labels(df_train_rul)\n",
    "df_train_labeled = scale_features(scaler_norm, df_train_labeled, id_cols+target_cols, 'train')\n",
    "\n",
    "df_test_cycles   = calc_maxcycles(df_test)\n",
    "df_test_cycles   = update_maxcycles(df_test_cycles, df_rul)\n",
    "df_test_rul      = calc_rul(df_test, df_test_cycles)\n",
    "df_test_labeled  = calc_labels(df_test_rul)\n",
    "df_test_labeled  = scale_features(scaler_norm, df_test_labeled, id_cols+target_cols, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Export of Dataset\n",
    "- DataSet should be a Numbered Time Series, with Engine as the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_path(export_path, partition_dir):\n",
    "    \"\"\"\n",
    "    Create the Partition dir if not existent\n",
    "    \"\"\"\n",
    "   \n",
    "    path = os.path.join(export_path, partition_dir)\n",
    "    [ os.makedirs(os.path.join(path, p)) for p in [\"features\", \"labels\"]\n",
    "      if not os.path.exists(os.path.join(path,p))]\n",
    "\n",
    "def create_sequenced_files(df, data_path, partition_dir):\n",
    "    \"\"\"\n",
    "    Export to CSV Files\n",
    "    Time Series records per File follows Numerical Inde according to Engine ID\n",
    "    \"\"\"\n",
    "    path = os.path.join(data_path, partition_dir)\n",
    "    create_path(data_path, partition_dir)\n",
    "    fname = \"turbofan-engine-ts-{}.csv\".format(df.id.unique()[0])\n",
    "    # filter out the output categories from the input categories\n",
    "    df_features = df.drop(['RUL', 'label1', 'label2'], axis=1)\n",
    "    df_labels   = df[['id', 'cycle', 'RUL', 'label1', 'label2']]\n",
    "    df_features.to_csv(os.path.join(os.path.join(path, \"features\"),fname), index=False)\n",
    "    df_labels.to_csv(os.path.join(os.path.join(path, \"labels\"),fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_data_path  = os.path.join(data_dir, \"exports/ckpts/data\")\n",
    "export_model_path = os.path.join(data_dir, \"exports/ckpts/models\") \n",
    "_ = df_train_labeled.groupby('id').apply(lambda fr: \n",
    "                                         create_sequenced_files(fr, export_data_path, 'train'))\n",
    "_ = df_test_labeled.groupby('id').apply(lambda fr:  \n",
    "                                        create_sequenced_files(fr, export_data_path, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "- Dimensions [samples, features, timesteps]\n",
    "  - samples    = num time series (batch_size)\n",
    "  - timesteps  = sequence length\n",
    "  - features   = values per timestep (setting{x}, cycle_norm, s1-s21}\n",
    "- Files\n",
    "  - Time Series in Single File:    Each record represents time series readings from a single engine\n",
    "  - Time Series in Multiple Files: Each file represents readings from a single engine\n",
    "- API\n",
    "  - Using Functional API rather an Sequential Model (for proper alignment with DL4J Model Import LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15631, 50, 25), (15631, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sequences via a lazy batch generator\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(df_id, seq_length, seq_cols):\n",
    "    \"\"\" \n",
    "    Only sequences that meet the window-length are considered, no padding is used. \n",
    "    This means for testing we need to drop those which are below the window-length. \n",
    "    An alternative would be to pad sequences so that we can use shorter ones \"\"\"\n",
    "    data_array = df_id[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]\n",
    "        \n",
    "def batch_gen(df, target, sequence_len):\n",
    "    \"\"\"\n",
    "    batch generator of (features, labels)\n",
    "    ===> basic method to just use .fit instead of lazy generator\n",
    "    \"\"\"\n",
    "    # filter columns\n",
    "    sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    # generate sequences and convert to numpy array\n",
    "    seq_gen = ( list(gen_sequence(df[df['id']==id], sequence_len, sequence_cols)) \n",
    "                for id in df_train['id'].unique() )\n",
    "    seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "    # generate labels and convert to numpy array\n",
    "    label_gen = [ gen_labels(df[df['id']==id], sequence_len, [target]) \n",
    "                  for id in df['id'].unique() ]\n",
    "    label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "    return seq_array, label_array\n",
    "    \n",
    "\n",
    "# pick the feature columns, set a large window size of 50 cycles \n",
    "sequence_length = 50\n",
    "seq_array, label_array = batch_gen(df_train_labeled, target='label1', sequence_len=sequence_length)\n",
    "nb_features, nb_out = seq_array.shape[2], label_array.shape[1]\n",
    "get_activation = lambda nb_neurons: 'softmax' if(nb_neurons > 1) else 'sigmoid'\n",
    "get_loss       = lambda nb_neurons: 'categorical_crossentropy' if(nb_neurons > 1) else 'binary_crossentropy' \n",
    "seq_array.shape, label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_layer(name, units, sequences, shape=None):\n",
    "    \"\"\"\n",
    "    name:  name given to layer\n",
    "    units: number of input units\n",
    "    shape: for initial layer input shape (only for first layer)\n",
    "    sequences: whether to return sequences or not\n",
    "    \"\"\"\n",
    "    data_dict = {\n",
    "     'name': name,\n",
    "     'units':units,\n",
    "     'return_sequences': sequences,\n",
    "     'kernel_initializer':    'glorot_uniform', \n",
    "     'recurrent_initializer': 'glorot_uniform'\n",
    "    }\n",
    "    if shape: data_dict.update({'input_shape':shape})\n",
    "    return LSTM(**data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputLayer_a (InputLayer)    (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "lstm_a (LSTM)                (None, 50, 164)           124640    \n",
      "_________________________________________________________________\n",
      "dropout_a (Dropout)          (None, 50, 164)           0         \n",
      "_________________________________________________________________\n",
      "lstm_b (LSTM)                (None, 50, 96)            100224    \n",
      "_________________________________________________________________\n",
      "dropout_b (Dropout)          (None, 50, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm_c (LSTM)                (None, 48)                27840     \n",
      "_________________________________________________________________\n",
      "dropout_c (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "outputLayer_a (Dense)        (None, 1)                 49        \n",
      "=================================================================\n",
      "Total params: 252,753\n",
      "Trainable params: 252,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Model the MLP via Sequential API Instead\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Activation\n",
    "\n",
    "def create_model_cg(sequence_len, num_features, num_classes):\n",
    "    \"\"\"\n",
    "    create a Computational Graph (Functional API)\n",
    "    sequence_len: length of sequence\n",
    "    num_features: feature vector shape (columns)\n",
    "    num_classses: number of classes    (target classifier)\n",
    "    \"\"\"\n",
    "    seq_in = Input(shape=(sequence_len, num_features), name=\"inputLayer_a\")\n",
    "    lstm_a = lstm_layer(name=\"lstm_a\", units=164, sequences=True)(seq_in)\n",
    "    dropout_a = Dropout(0.2, name='dropout_a')(lstm_a)\n",
    "\n",
    "    lstm_b = lstm_layer(name=\"lstm_b\", units=96, sequences=True)(dropout_a)\n",
    "    dropout_b = Dropout(0.2, name='dropout_b')(lstm_b)\n",
    "    \n",
    "    lstm_c = lstm_layer(name=\"lstm_c\", units=48, sequences=False)(dropout_b)\n",
    "    dropout_c = Dropout(0.2, name='dropout_c')(lstm_c)\n",
    "\n",
    "    output = Dense(num_classes, activation=get_activation(num_classes), name='outputLayer_a')(dropout_c)\n",
    "    model   = Model(inputs=seq_in, outputs=output, name=\"Keras_LSTM\")\n",
    "    return model\n",
    "\n",
    "def create_model_mlp(sequence_len, num_features, num_classes):\n",
    "    \"\"\"\n",
    "    create a Sequential MLP Model\n",
    "    sequence_len: length of sequence\n",
    "    num_features: feature vector shape (columns)\n",
    "    num_classses: number of classes    (target classifier)\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(lstm_layer(name=\"lstm_a\", units=164, sequences=True, shape=(sequence_len, num_features)))\n",
    "    model.add(Dropout(0.2, name=\"dropout_a\"))\n",
    "\n",
    "    model.add(lstm_layer(name=\"lstm_b\", units=96,  sequences=True))\n",
    "    model.add(Dropout(0.2, name=\"dropout_b\"))\n",
    "\n",
    "    model.add(lstm_layer(name=\"lstm_c\", units=48,  sequences=False))\n",
    "    model.add(Dropout(0.2, name=\"dropout_c\"))\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation=get_activation(num_classes), name=\"outputLayer_a\"))\n",
    "    return model\n",
    "\n",
    "model_type = \"compgraph\"\n",
    "#model = create_model_mlp(sequence_length, nb_features, nb_out)    \n",
    "model = create_model_cg(sequence_length, nb_features, nb_out)\n",
    "model.compile(loss=get_loss(nb_out), optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training + Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12504 samples, validate on 3127 samples\n",
      "Epoch 1/10\n",
      "12504/12504 [==============================] - 225s - loss: 0.1143 - acc: 0.9516 - val_loss: 0.0707 - val_acc: 0.9722\n",
      "Epoch 2/10\n",
      "12504/12504 [==============================] - 217s - loss: 0.0678 - acc: 0.9710 - val_loss: 0.0494 - val_acc: 0.9799\n",
      "Epoch 3/10\n",
      "12504/12504 [==============================] - 214s - loss: 0.0593 - acc: 0.9753 - val_loss: 0.0539 - val_acc: 0.9747\n",
      "Epoch 4/10\n",
      "12504/12504 [==============================] - 200s - loss: 0.0472 - acc: 0.9787 - val_loss: 0.0534 - val_acc: 0.9767\n",
      "CPU times: user 22min 34s, sys: 8min 1s, total: 30min 35s\n",
      "Wall time: 14min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.callbacks import EarlyStopping\n",
    "# fit the network\n",
    "# does not shuffle by default\n",
    "model.fit(seq_array, label_array, epochs=10, batch_size=32, verbose=1,\n",
    "          validation_split=0.2,\n",
    "          callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                     patience=1, verbose=0, mode='auto')])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD: Peform evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(export_model_path): os.makedirs(export_model_path)\n",
    "serialize_str = lambda path, s, arg: os.path.join(path, s.format(arg))\n",
    "    \n",
    "model.save(serialize_str(export_model_path, \"model_keras_{}.h5\", model_type))\n",
    "model.save_weights(serialize_str(export_model_path, \"model_keras_weights_{}.h5\", model_type))\n",
    "with open(serialize_str(export_model_path,  \"model_keras_config_{}.h5\", model_type), 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "with open(serialize_str(export_model_path,  \"layers_keras_config_{}.h5\", model_type), 'w') as f:\n",
    "    json.dumps(model.get_config(), f)                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
